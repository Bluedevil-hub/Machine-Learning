{"cells":[{"cell_type":"markdown","metadata":{"id":"NJfajcbpQmvY"},"source":["## Assignment : 14"]},{"cell_type":"markdown","metadata":{"id":"Q5mBKTHcQmvj"},"source":["<pre>\n","1. You can work with preprocessed_data.csv for the assignment. You can get the data from - <a href='https://drive.google.com/drive/u/0/folders/1CJnItndeSSJu7aragQoXWZS9-0apN6pp'>Data folder </a>\n","2. Load the data in your notebook.\n","3. After step 2 you have to train 3 types of models as discussed below. \n","4. For all the model use <a href='https://scikit-learn.org/stable/modules/model_evaluation.html#roc-metrics'>'auc'</a> as a metric. check <a  href='https://stackoverflow.com/a/46844409'>this</a> and <a  href='https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807'>this</a> for using auc as a metric \n","5. You are free to choose any number of layers/hiddden units but you have to use same type of architectures shown below. \n","6. You can use any one of the optimizers and choice of Learning rate and momentum.\n","7. For all the model's use <a href='https://www.youtube.com/watch?v=2U6Jl7oqRkM'>TensorBoard</a> and plot the Metric value and Loss with epoch. While submitting, take a screenshot of plots and include those images in a separate pad and write your observations about them.\n","8. Make sure that you are using GPU to train the given models.\n","</pre>"]},{"cell_type":"markdown","source":["#Imports"],"metadata":{"id":"dBfbDBSGlUG6"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from sklearn.model_selection import train_test_split\n","import pickle\n","import tensorflow\n","from tensorflow.keras.layers import Input,Dense,LSTM,Flatten\n","from tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import StandardScaler\n","from numpy import zeros\n","from tensorflow.keras.layers import Dense,concatenate,Activation,Dropout,Input\n","from tensorflow.keras.models import Model"],"metadata":{"id":"ewa6WP5gQuPr","executionInfo":{"status":"ok","timestamp":1655237734326,"user_tz":-330,"elapsed":395,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":148,"outputs":[]},{"cell_type":"markdown","source":["#Data"],"metadata":{"id":"ull4A7ntR_7Y"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ghUOrgdqQmvm"},"outputs":[],"source":["#you can use gdown modules to import dataset for the assignment\n","#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n","#you can run the below cell to import the required preprocessed data.csv file and glove vector"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"6Uhk_HC6Qmvq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655228995905,"user_tz":-330,"elapsed":6536,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"dd722639-88f0-421f-b98b-d00ff534753d"},"outputs":[{"output_type":"stream","name":"stdout","text":["/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n","To: /content/preprocessed_data.csv\n","100% 124M/124M [00:00<00:00, 140MB/s]\n","/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n","  category=FutureWarning,\n","Downloading...\n","From: https://drive.google.com/uc?id=1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_\n","To: /content/glove_vectors\n","100% 128M/128M [00:01<00:00, 110MB/s] \n"]}],"source":["#you can use gdown modules to import dataset for the assignment\n","#for importing any file from drive to Colab you can write the syntax as !gdown --id file_id\n","#you can run the below cell to import the required preprocessed data.csv file and glove vector\n","\n","!gdown --id 1GpATd_pM4mcnWWIs28-s1lgqdAg2Wdv-\n","!gdown --id 1pGd5tLwA30M7wkbJKdXHaae9tYVDICJ_"]},{"cell_type":"markdown","source":["##Preprocessing"],"metadata":{"id":"KQHf9eWKSCQt"}},{"cell_type":"markdown","metadata":{"id":"AY0pFx4iQmvr"},"source":["## <font color='red'> Model-1 </font>\n","Build and Train deep neural network as shown below"]},{"cell_type":"markdown","metadata":{"id":"-Er11vBDQmvs"},"source":["<img src='https://i.imgur.com/w395Yk9.png'>\n","ref: https://i.imgur.com/w395Yk9.png"]},{"cell_type":"markdown","metadata":{"id":"Uz7Ksys-Qmvt"},"source":["- __Input_seq_total_text_data__ --- You have to give Total text data columns. After this use the Embedding layer to get word vectors. Use given predefined glove word vectors, don't train any word vectors. After this use LSTM and get the LSTM output and Flatten that output. \n","- __Input_school_state__ --- Give 'school_state' column as input to embedding layer and Train the Keras Embedding layer. \n","- __Project_grade_category__  --- Give 'project_grade_category' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_clean_categories__ --- Give 'input_clean_categories' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_clean_subcategories__ --- Give 'input_clean_subcategories' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_clean_subcategories__ --- Give 'input_teacher_prefix' column as input to embedding layer and Train the Keras Embedding layer.\n","- __Input_remaining_teacher_number_of_previously_posted_projects._resource_summary_contains_numerical_digits._price._quantity__ ---concatenate remaining columns and add a Dense layer after that. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"tbxjI7wmQmvv"},"source":["Below is an example of embedding layer for a categorical columns. In below code all are dummy values, we gave only for referance. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bdBxzHKXQmvy"},"outputs":[],"source":["# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n","input_layer = Input(shape=(n,))\n","embedding = Embedding(no_1, no_2, input_length=n)(input_layer)\n","flatten = Flatten()(embedding)"]},{"cell_type":"markdown","metadata":{"id":"BpzGLz69Qmv0"},"source":["### 1. Go through this blog, if you have any doubt on using predefined Embedding values in Embedding layer - https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n","### 2. Please go through this link https://keras.io/getting-started/functional-api-guide/ and check the 'Multi-input and multi-output models' then you will get to know how to give multiple inputs. "]},{"cell_type":"markdown","metadata":{"id":"2em1Z9hbQmv2"},"source":["# <font color='red'> Model-1 </font>"]},{"cell_type":"markdown","source":["##Data Splitt"],"metadata":{"id":"nLky8lm56LFF"}},{"cell_type":"code","source":["#https://www.youtube.com/watch?v=Fuw0wv3X-0o&list=PLeo1K3hjS3uu7CxAacxVndI4bE_o3BDtO&index=40&ab_channel=codebasics"],"metadata":{"id":"7-Hm6W2pD9TE","executionInfo":{"status":"ok","timestamp":1655229024035,"user_tz":-330,"elapsed":381,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","execution_count":182,"metadata":{"id":"HG33YzNKQmv3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655240781511,"user_tz":-330,"elapsed":2943,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"68baf549-041f-4a7d-91dc-cdeea802ebb7"},"outputs":[{"output_type":"stream","name":"stdout","text":["(109248, 9)\n"]}],"source":["#read the csv file\n","df = pd.read_csv('preprocessed_data.csv')\n","print(df.shape)"]},{"cell_type":"code","source":["df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9bnPpoxg56tX","executionInfo":{"status":"ok","timestamp":1655229024420,"user_tz":-330,"elapsed":12,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"49891d3f-a743-4132-8737-52344039ef7a"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['school_state', 'teacher_prefix', 'project_grade_category',\n","       'teacher_number_of_previously_posted_projects', 'project_is_approved',\n","       'clean_categories', 'clean_subcategories', 'essay', 'price'],\n","      dtype='object')"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["- Text Features\n"," - essay\n","\n","- Categorical Features\n","\n","  - school_state\n","  - teacher_prefix\n","  - project_grade_category\n","  - clean_categories\n","  - clean_sub_categories\n","\n","- Numerical Features:\n","  - teacher_number_of_previously_posted_projects\n","  - Price"],"metadata":{"id":"2RspKDnX6C6I"}},{"cell_type":"code","source":["y = df['project_is_approved']\n","X = df.drop(columns=['project_is_approved'])\n","print('X shape:', X.shape,'y shape:',y.shape )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qHHWBEm86IY9","executionInfo":{"status":"ok","timestamp":1655240785932,"user_tz":-330,"elapsed":379,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"6f8cfd0c-fbd4-4bc9-a2af-9d9ad2ee9c52"},"execution_count":183,"outputs":[{"output_type":"stream","name":"stdout","text":["X shape: (109248, 8) y shape: (109248,)\n"]}]},{"cell_type":"code","execution_count":184,"metadata":{"id":"ACOjrtXsQmv4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655240788989,"user_tz":-330,"elapsed":613,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"3161e1e9-9ead-4e93-aa37-eca6dd5f0ff4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Train: (69918, 8) (69918,)\n","CV: (17480, 8) (17480,)\n","Test: (21850, 8) (21850,)\n"]}],"source":["# perform stratified train test split on the dataset\n","X_train,X_test,y_train,y_test = train_test_split(X,y,stratify=y,test_size= 0.2)\n","X_train,X_cv,y_train,y_cv = train_test_split(X_train,y_train,stratify=y_train,test_size= 0.2)\n","\n","print('Train:',X_train.shape,y_train.shape)\n","print('CV:',X_cv.shape,y_cv.shape)\n","print('Test:',X_test.shape,y_test.shape)"]},{"cell_type":"markdown","metadata":{"id":"xJbOczJQQmv5"},"source":["## 1.1 Text Vectorization"]},{"cell_type":"code","execution_count":185,"metadata":{"id":"COynohg4Qmv6","executionInfo":{"status":"ok","timestamp":1655240792789,"user_tz":-330,"elapsed":360,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"outputs":[],"source":["#since the data is already preprocessed, we can directly move to vectorization part\n","#first we will vectorize the text data\n","#for vectorization of text data in deep learning we use tokenizer, you can go through below references\n","# https://www.kdnuggets.com/2020/03/tensorflow-keras-tokenization-text-data-prep.html\n","#https://stackoverflow.com/questions/51956000/what-does-keras-tokenizer-method-exactly-do\n","# after text vectorization you should get train_padded_docs and test_padded_docs"]},{"cell_type":"code","source":["#Notes\n","#train_data = X_train['essay']\n","#test_data = X_test['essay']\n","#print(train_data.shape,test_data.shape)\n","\n","#oov_token = '<UNK>'\n","#pad_type = 'post'\n","#trunc_type = 'post'\n","\n","#tokenizer = Tokenizer( oov_token=oov_token)\n","\n","#Converts the words to numbers ie each number is converted to number\n","#tokenizer.fit_on_texts(train_data)\n","#print('Vocab _length ',len(tokenizer.word_index))\n","\n","#Transform the sent to list of number of sent with help of above\n","#train_sequences = tokenizer.texts_to_sequences(train_data)\n","#print('train_seq len',len(train_sequences))\n","\n","#As the sent are of diff length we need to pad them with max\n","#maxlen = max([len(x) for x in train_sequences])\n","#print('max len',maxlen)\n","\n","#Converts to matrix\n","#train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n","#print('train_padded shape ',train_padded.shape)\n","\n","#Glove Embedding:\n","#   docs = ['the is best work','keep the  work ']\n","# tokenizer = Tokenizer( oov_token=oov_token)\n","#   tokenizer.fit_on_texts(docs)   # fIts the docs\n","# train = tokenizer.texts_to_sequences(docs) # Converts the doc to seq [[2, 4, 5, 3], [6, 2, 3]]\n","\n","#   maxlen = max([len(x) for x in train])   get max length for padd in thsi 4\n","#   tokenizer.word_index  \n","#    gives dic {'<UNK>': 1, 'best': 5, 'is': 4, 'keep': 6, 'the': 2, 'work': 3}\n","\n","#with open('/content/glove_vectors', 'rb') as f:\n","    #glove = pickle.load(f)   # opens glove as dict of word and vectors of 300\n","#  vocab_size = len(tokenizer.word_index)+1  +1 as while indexing embedding mat last digit is not included\n","#  embedding_matrix = np.zeros((vocab_size, 300))   shape of 7,300\n","#  for word,i in tokenizer.word_index.items():\n","#   if i< vocab_size :\n","#   emb_vec=  glove.get(word)\n","#    if emb_vec is not None:\n","#      embedding_matrix[i] = emb_vec   # just gives the mat with word vectors\n","# embedding_matrix[6][:10]   glove.get('keep')[:10]\n","# Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=4, trainable=False)\n","# 100 is vectro size, input_length is max_lenght used for padding"],"metadata":{"id":"CuaAcCCs--9R","executionInfo":{"status":"ok","timestamp":1655240793178,"user_tz":-330,"elapsed":11,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":186,"outputs":[]},{"cell_type":"code","source":["#Essay \n","\n","oov_token = '<UNK>'\n","pad_type = 'post'\n","trunc_type = 'post' \n","\n","tokenizer = Tokenizer( oov_token=oov_token)\n","tokenizer.fit_on_texts(X_train['essay'].tolist())\n","\n","train_sequences = tokenizer.texts_to_sequences(X_train['essay'])\n","cv_sequences = tokenizer.texts_to_sequences(X_cv['essay'])\n","test_sequences = tokenizer.texts_to_sequences(X_test['essay'])\n","\n","maxlen = max([len(x) for x in train_sequences])\n","\n","train_padded = pad_sequences(train_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n","cv_padded = pad_sequences(cv_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n","test_padded = pad_sequences(test_sequences, padding=pad_type, truncating=trunc_type, maxlen=maxlen)\n","\n","print('Train padded',train_padded.shape)\n","print('cv padded',cv_padded.shape)\n","print('Test padded',test_padded.shape)\n","print('max_len',maxlen)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mciIIe7jspSH","executionInfo":{"status":"ok","timestamp":1655240845981,"user_tz":-330,"elapsed":52812,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"ff8a0ee4-36f5-4ee4-cfda-24c00b762745"},"execution_count":187,"outputs":[{"output_type":"stream","name":"stdout","text":["Train padded (69918, 339)\n","cv padded (17480, 339)\n","Test padded (21850, 339)\n","max_len 339\n"]}]},{"cell_type":"code","execution_count":188,"metadata":{"id":"BfP9hLRJQmv7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1655240846533,"user_tz":-330,"elapsed":592,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"d59a7e35-28d9-42c9-acbe-b86512207ea0"},"outputs":[{"output_type":"stream","name":"stdout","text":["47267\n","(47267, 300)\n"]}],"source":["#after getting the padded_docs you have to use predefined glove vectors to get 300 dim representation for each word\n","# we will be storing this data in form of an embedding matrix and will use it while defining our model\n","# Please go through following blog's 'Example of Using Pre-Trained GloVe Embedding' section to understand \n","#how to create embedding matrix\n","# https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/\n","#Now we have to convert the 640 298 train to glove rep of 640 298 300 as glove vectros are of 300\n","\n","with open('/content/glove_vectors', 'rb') as f:\n","    glove = pickle.load(f)\n","\n","vocab_size = len(tokenizer.word_index)+1\n","print(vocab_size)\n","\n","embedding_matrix = np.zeros((vocab_size, 300))\n","\n","for word,i in tokenizer.word_index.items():\n","  if i< vocab_size :\n","    emb_vec=  glove.get(word)\n","    if emb_vec is not None:\n","      embedding_matrix[i] = emb_vec\n","\n","print(embedding_matrix.shape)"]},{"cell_type":"code","source":["maxlen"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ja_NpOlfq96U","executionInfo":{"status":"ok","timestamp":1655240846535,"user_tz":-330,"elapsed":13,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"d25d5852-db96-46b1-8612-ce237a49a5ab"},"execution_count":189,"outputs":[{"output_type":"execute_result","data":{"text/plain":["339"]},"metadata":{},"execution_count":189}]},{"cell_type":"code","source":["Input_essay = Input(shape = maxlen,name='Input_essay')\n","essay_emb = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=maxlen, trainable=False)(Input_essay)\n","essay_lstm = LSTM(units= 50,return_sequences=True)(essay_emb) \n","\n","essay_layer = Flatten()(essay_lstm)"],"metadata":{"id":"t7-00890qGxi","executionInfo":{"status":"ok","timestamp":1655240847337,"user_tz":-330,"elapsed":808,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":190,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XoBFPa_9Qmv8"},"source":["## 1.2 Categorical feature Vectorization"]},{"cell_type":"code","execution_count":191,"metadata":{"id":"Z3S2HukgQmv9","executionInfo":{"status":"ok","timestamp":1655240852928,"user_tz":-330,"elapsed":421,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"outputs":[],"source":["# for model 1 and model 2, we have to assign a unique number to each feature in a particular categorical column.\n","# you can either use tokenizer,label encoder or ordinal encoder to perform the task\n","# label encoder gives an error for 'unseen values' (values present in test but not in train)\n","# handle unseen values with label encoder - https://stackoverflow.com/a/56876351\n","# ordinal encoder also gives error with unseen values but you can use modify handle_unknown parameter\n","# documentation of ordianl encoder https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html\n","# after categorical feature vectorization you will have column_train_data and column_test_data.\n"]},{"cell_type":"code","source":["def token(col,X_train,X_cv,X_test):\n","  tokenizer = Tokenizer(oov_token='<UNK>')\n","  tokenizer.fit_on_texts(X_train[col].to_list()) \n","  distinct_words = len(tokenizer.word_index) \n","  \n","  t_train = tokenizer.texts_to_sequences(X_train[col])\n","  t_cv = tokenizer.texts_to_sequences(X_cv[col])\n","  t_test = tokenizer.texts_to_sequences(X_test[col])\n","\n","  maxlen = max([len(x) for x in t_train])\n","\n","  train_pad = pad_sequences(t_train, maxlen=maxlen)\n","  cv_pad = pad_sequences(t_cv, maxlen=maxlen)\n","  test_pad = pad_sequences(t_test, maxlen=maxlen)\n","\n","  return train_pad,cv_pad,test_pad,maxlen,distinct_words\n"],"metadata":{"id":"WV-aVhgUz3W0","executionInfo":{"status":"ok","timestamp":1655240853302,"user_tz":-330,"elapsed":11,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":192,"outputs":[]},{"cell_type":"code","source":["train_pad_ss,cv_pad_ss,test_pad_ss,max_ss,ss_words = token('school_state',X_train,X_cv,X_test)\n","train_pad_tp,cv_pad_tp,test_pad_tp,max_tp,tp_words = token('teacher_prefix',X_train,X_cv,X_test)\n","train_pad_pgc,cv_pad_pgc,test_pad_pgc,max_pgc,pgc_words = token('project_grade_category',X_train,X_cv,X_test)\n","train_pad_cc,cv_pad_cc,test_pad_cc,max_cc,cc_words = token('clean_categories',X_train,X_cv,X_test)\n","train_pad_cs,cv_pad_cs,test_pad_cs,max_cs,cs_words = token('clean_subcategories',X_train,X_cv,X_test)\n"],"metadata":{"id":"gduqSaHu1JMs","executionInfo":{"status":"ok","timestamp":1655240862703,"user_tz":-330,"elapsed":9410,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":193,"outputs":[]},{"cell_type":"code","source":["ss_words\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wt528Khn3Yt4","executionInfo":{"status":"ok","timestamp":1655240862704,"user_tz":-330,"elapsed":26,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"c0b2c2aa-81ec-4c0d-f029-45540775b936"},"execution_count":194,"outputs":[{"output_type":"execute_result","data":{"text/plain":["52"]},"metadata":{},"execution_count":194}]},{"cell_type":"code","source":["#school_state\n","Input_ss = Input(shape = max_ss,name='Input_ss')\n","ss_emd = Embedding(input_dim=ss_words,output_dim = 2,input_length= max_ss)(Input_ss)\n","ss_lstm = LSTM(units= 50,return_sequences=True)(ss_emd) \n","ss_layer = Flatten()(ss_lstm)\n","\n","#teacher_prefix\n","Input_tp = Input(shape = max_tp,name='Input_tp')\n","tp_emd = Embedding(input_dim=tp_words,output_dim=2,input_length=max_tp)(Input_tp)\n","tp_lstm = LSTM(units= 50,return_sequences=True)(tp_emd) \n","tp_layer = Flatten()(tp_lstm)\n","\n","#project_grade_category\n","Input_pgc = Input(shape = max_pgc,name='Input_pgc')\n","pgc_emd = Embedding(input_dim=pgc_words, output_dim=5,input_length=max_pgc)(Input_pgc)\n","pgc_lstm = LSTM(units= 50,return_sequences=True)(pgc_emd) \n","pgc_layer = Flatten()(pgc_lstm)\n","\n","#clean_categories\n","Input_cc = Input(shape = max_cc,name='Input_cc')\n","cc_emd = Embedding(input_dim=cc_words,output_dim=5,input_length=max_cc)(Input_cc)\n","cc_lstm = LSTM(units= 50,return_sequences=True)(cc_emd) \n","cc_layer = Flatten()(cc_lstm)\n","\n","#clean_sub_categories\n","Input_cs = Input(shape = max_cs,name='Input_cs')\n","cs_emd = Embedding(input_dim=cs_words,output_dim=5,input_length=max_cs)(Input_cs)\n","cs_lstm = LSTM(units= 50,return_sequences=True)(cs_emd) \n","cs_layer = Flatten()(cs_lstm)\n"],"metadata":{"id":"J7s1a6Bdix44","executionInfo":{"status":"ok","timestamp":1655241005310,"user_tz":-330,"elapsed":3224,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":202,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"8KzTe8zn3D6B","executionInfo":{"status":"ok","timestamp":1655240864410,"user_tz":-330,"elapsed":29,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":195,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2mvLqp2IQmv_"},"source":["## 1.3 Numerical feature Vectorization"]},{"cell_type":"code","execution_count":196,"metadata":{"id":"L4Qc-Vh4QmwA","executionInfo":{"status":"ok","timestamp":1655240864412,"user_tz":-330,"elapsed":27,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"outputs":[],"source":["# you have to standardise the numerical columns\n","# stack both the numerical features\n","#after numerical feature vectorization you will have numerical_data_train and numerical_data_test"]},{"cell_type":"code","execution_count":197,"metadata":{"id":"KPN0pmdWQmwA","executionInfo":{"status":"ok","timestamp":1655240864413,"user_tz":-330,"elapsed":26,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"outputs":[],"source":["def num_encode(col,X_train,X_cv,X_testt):\n","  scaler = StandardScaler()\n","\n","  train = scaler.fit_transform(X_train[col].values.reshape(-1,1))\n","  cv = scaler.transform(X_cv[col].values.reshape(-1,1))\n","  test = scaler.transform(X_test[col].values.reshape(-1,1))\n","  return train, cv,test\n","\n","X_train_price,X_cv_price ,X_test_price = num_encode('price',X_train,X_cv,X_test)\n","X_train_tnp,X_cv_tnp ,X_test_tnp = num_encode('teacher_number_of_previously_posted_projects',X_train,X_cv,X_test)\n","\n","train_num = np.concatenate((X_train_price,X_train_tnp),axis=1)\n","cv_num = np.concatenate((X_cv_price,X_cv_tnp),axis=1)\n","test_num = np.concatenate((X_test_price,X_test_tnp),axis=1)"]},{"cell_type":"code","source":["num_col = Input(shape=(2,),name='num_col')\n","dense_num = Dense(8,activation='relu')(num_col)\n"],"metadata":{"id":"MJJy1vyO0LlA","executionInfo":{"status":"ok","timestamp":1655240864414,"user_tz":-330,"elapsed":26,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":198,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-GAGVFSoQmwB"},"source":["## 1.4 Defining the model"]},{"cell_type":"markdown","metadata":{"id":"tZKSkqqfQmwC"},"source":["<img src='https://i.imgur.com/w395Yk9.png'>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cwEuxFsLQmwD"},"outputs":[],"source":["# as of now we have vectorized all our features now we will define our model.\n","# as it is clear from above image that the given model has multiple input layers and hence we have to use functional API\n","# Please go through - https://keras.io/guides/functional_api/\n","# it is a good programming practise to define your complete model i.e all inputs , intermediate and output layers at one place.\n","# while defining your model make sure that you use variable names while defining any length,dimension or size.\n","#for ex.- you should write the code as 'input_text = Input(shape=(pad_length,))' and not as 'input_text = Input(shape=(300,))'\n","# the embedding layer for text data should be non trainable\n","# the embedding layer for categorical data should be trainable\n","# https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n","# https://towardsdatascience.com/deep-embeddings-for-categorical-variables-cat2vec-b05c8ab63ac0\n","#print model.summary() after you have defined the model\n","#plot the model using utils.plot_model module and make sure that it is similar to the above image"]},{"cell_type":"code","source":["#concat laerys\n","concat = concatenate([essay_layer,ss_layer,tp_layer,pgc_layer,cc_layer,cs_layer,dense_num])\n","\n","dense1 = Dense(128,activation=\"relu\")(concat)\n","drop1 =Dropout(0.5)(dense1)\n","dense2 = Dense(256,activation=\"relu\")(drop1)\n","drop2 =Dropout(0.5)(dense2)\n","dense3 = Dense(128,activation=\"relu\")(drop2)\n","\n","output = Dense(2, activation='softmax', name='output')(dense3)\n","\n","model = Model(inputs=[Input_essay,Input_ss,Input_tp,Input_pgc,Input_cc,Input_cs,num_col],outputs=output)\n","\n","\n","model.compile(loss='categorical_crossentropy',  metrics=['accuracy'])\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLyi52Dw1qqH","executionInfo":{"status":"ok","timestamp":1655241071951,"user_tz":-330,"elapsed":1057,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"274c15ad-4433-457a-8ba1-0986f3bf180d"},"execution_count":204,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_6\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," Input_essay (InputLayer)       [(None, 339)]        0           []                               \n","                                                                                                  \n"," Input_ss (InputLayer)          [(None, 1)]          0           []                               \n","                                                                                                  \n"," Input_tp (InputLayer)          [(None, 1)]          0           []                               \n","                                                                                                  \n"," Input_pgc (InputLayer)         [(None, 3)]          0           []                               \n","                                                                                                  \n"," Input_cc (InputLayer)          [(None, 5)]          0           []                               \n","                                                                                                  \n"," Input_cs (InputLayer)          [(None, 5)]          0           []                               \n","                                                                                                  \n"," embedding_10 (Embedding)       (None, 339, 300)     14180100    ['Input_essay[0][0]']            \n","                                                                                                  \n"," embedding_16 (Embedding)       (None, 1, 2)         104         ['Input_ss[0][0]']               \n","                                                                                                  \n"," embedding_17 (Embedding)       (None, 1, 2)         12          ['Input_tp[0][0]']               \n","                                                                                                  \n"," embedding_18 (Embedding)       (None, 3, 5)         50          ['Input_pgc[0][0]']              \n","                                                                                                  \n"," embedding_19 (Embedding)       (None, 5, 5)         80          ['Input_cc[0][0]']               \n","                                                                                                  \n"," embedding_20 (Embedding)       (None, 5, 5)         190         ['Input_cs[0][0]']               \n","                                                                                                  \n"," lstm_8 (LSTM)                  (None, 339, 50)      70200       ['embedding_10[0][0]']           \n","                                                                                                  \n"," lstm_14 (LSTM)                 (None, 1, 50)        10600       ['embedding_16[0][0]']           \n","                                                                                                  \n"," lstm_15 (LSTM)                 (None, 1, 50)        10600       ['embedding_17[0][0]']           \n","                                                                                                  \n"," lstm_16 (LSTM)                 (None, 3, 50)        11200       ['embedding_18[0][0]']           \n","                                                                                                  \n"," lstm_17 (LSTM)                 (None, 5, 50)        11200       ['embedding_19[0][0]']           \n","                                                                                                  \n"," lstm_18 (LSTM)                 (None, 5, 50)        11200       ['embedding_20[0][0]']           \n","                                                                                                  \n"," num_col (InputLayer)           [(None, 2)]          0           []                               \n","                                                                                                  \n"," flatten_7 (Flatten)            (None, 16950)        0           ['lstm_8[0][0]']                 \n","                                                                                                  \n"," flatten_13 (Flatten)           (None, 50)           0           ['lstm_14[0][0]']                \n","                                                                                                  \n"," flatten_14 (Flatten)           (None, 50)           0           ['lstm_15[0][0]']                \n","                                                                                                  \n"," flatten_15 (Flatten)           (None, 150)          0           ['lstm_16[0][0]']                \n","                                                                                                  \n"," flatten_16 (Flatten)           (None, 250)          0           ['lstm_17[0][0]']                \n","                                                                                                  \n"," flatten_17 (Flatten)           (None, 250)          0           ['lstm_18[0][0]']                \n","                                                                                                  \n"," dense_16 (Dense)               (None, 8)            24          ['num_col[0][0]']                \n","                                                                                                  \n"," concatenate_8 (Concatenate)    (None, 17708)        0           ['flatten_7[0][0]',              \n","                                                                  'flatten_13[0][0]',             \n","                                                                  'flatten_14[0][0]',             \n","                                                                  'flatten_15[0][0]',             \n","                                                                  'flatten_16[0][0]',             \n","                                                                  'flatten_17[0][0]',             \n","                                                                  'dense_16[0][0]']               \n","                                                                                                  \n"," dense_20 (Dense)               (None, 128)          2266752     ['concatenate_8[0][0]']          \n","                                                                                                  \n"," dropout_12 (Dropout)           (None, 128)          0           ['dense_20[0][0]']               \n","                                                                                                  \n"," dense_21 (Dense)               (None, 256)          33024       ['dropout_12[0][0]']             \n","                                                                                                  \n"," dropout_13 (Dropout)           (None, 256)          0           ['dense_21[0][0]']               \n","                                                                                                  \n"," dense_22 (Dense)               (None, 128)          32896       ['dropout_13[0][0]']             \n","                                                                                                  \n"," output (Dense)                 (None, 2)            258         ['dense_22[0][0]']               \n","                                                                                                  \n","==================================================================================================\n","Total params: 16,638,490\n","Trainable params: 2,458,390\n","Non-trainable params: 14,180,100\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"kdVBw94JAjKq","executionInfo":{"status":"ok","timestamp":1655240881144,"user_tz":-330,"elapsed":338,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}}},"execution_count":200,"outputs":[]},{"cell_type":"code","source":["train_data = [train_padded,train_pad_ss,train_pad_tp,train_pad_pgc,train_pad_cc,train_pad_cs,train_num]\n","cv_data = [cv_padded,cv_pad_ss,cv_pad_tp,cv_pad_pgc,cv_pad_cc,cv_pad_cs,cv_num]\n","train_data = [test_padded,test_pad_ss,test_pad_tp,test_pad_pgc,test_pad_cc,test_pad_cs,test_num]\n","\n","\n","model_1 = model.fit([train_padded,train_pad_ss,train_pad_tp,train_pad_pgc,train_pad_cc,train_pad_cs,train_num],to_categorical(y_train),\n","                        epochs=2,verbose=1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"VtTTkjQY1qah","executionInfo":{"status":"error","timestamp":1655241164115,"user_tz":-330,"elapsed":74669,"user":{"displayName":"Harshil Mehta","userId":"03602023061621504968"}},"outputId":"ce4233e3-db42-4e49-d106-ca566773df7e"},"execution_count":205,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/2\n"]},{"output_type":"error","ename":"InvalidArgumentError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)","\u001b[0;32m<ipython-input-205-f147c8ab7ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m model_1 = model.fit([train_padded,train_pad_ss,train_pad_tp,train_pad_pgc,train_pad_cc,train_pad_cs,train_num],to_categorical(y_train),\n\u001b[0;32m----> 7\u001b[0;31m                         epochs=2,verbose=1)\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'model_6/embedding_18/embedding_lookup' defined at (most recent call last):\n    File \"/usr/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n      \"__main__\", mod_spec)\n    File \"/usr/lib/python3.7/runpy.py\", line 85, in _run_code\n      exec(code, run_globals)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n      app.launch_new_instance()\n    File \"/usr/local/lib/python3.7/dist-packages/traitlets/config/application.py\", line 846, in launch_instance\n      app.start()\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelapp.py\", line 499, in start\n      self.io_loop.start()\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 132, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 541, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.7/asyncio/base_events.py\", line 1786, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.7/asyncio/events.py\", line 88, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/platform/asyncio.py\", line 122, in _handle_events\n      handler_func(fileobj, events)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 577, in _handle_events\n      self._handle_recv()\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 606, in _handle_recv\n      self._run_callback(callback, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/zmq/eventloop/zmqstream.py\", line 556, in _run_callback\n      callback(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/tornado/stack_context.py\", line 300, in null_wrapper\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n      return self.dispatch_shell(stream, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n      handler(stream, idents, msg)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n      user_expressions, allow_stdin)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"/usr/local/lib/python3.7/dist-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n      return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n      interactivity=interactivity, compiler=compiler, result=result)\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n      if self.run_code(code, result):\n    File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"<ipython-input-205-f147c8ab7ace>\", line 7, in <module>\n      epochs=2,verbose=1)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1384, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1021, in train_function\n      return step_function(self, iterator)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1010, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 1000, in run_step\n      outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\", line 859, in train_step\n      y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 452, in call\n      inputs, training=training, mask=mask)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\", line 589, in _run_internal_graph\n      outputs = node.layer(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 64, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\", line 1096, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\", line 92, in error_handler\n      return fn(*args, **kwargs)\n    File \"/usr/local/lib/python3.7/dist-packages/keras/layers/embeddings.py\", line 197, in call\n      out = tf.nn.embedding_lookup(self.embeddings, inputs)\nNode: 'model_6/embedding_18/embedding_lookup'\nindices[3,2] = 10 is not in [0, 10)\n\t [[{{node model_6/embedding_18/embedding_lookup}}]] [Op:__inference_train_function_48810]"]}]},{"cell_type":"markdown","metadata":{"id":"rvOAcZmVQmwF"},"source":["## 1.5 Compiling and fititng your model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3NJGliLQmwG"},"outputs":[],"source":["#define custom auc as metric , do not use tf.keras.metrics\n","# https://stackoverflow.com/a/46844409 - custom AUC reference 1\n","# https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/80807  - custom AUC reference 2\n","# compile and fit your model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SHrONQzQmwG"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1WfY1V2yQmwH"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"gMlQRVZdQmwI"},"source":["# <font color='red'> Model-2 </font>"]},{"cell_type":"markdown","metadata":{"id":"7-ONwnSpQmwI"},"source":["Use the same model as above but for 'input_seq_total_text_data' give only some words in the sentance not all the words. Filter the words as below. "]},{"cell_type":"markdown","metadata":{"id":"Kd99AnIfQmwJ"},"source":["<pre>\n","1. Fit TF-IDF vectorizer on the Train data <br>\n","2. Get the idf value for each word we have in the train data. Please go through <a  href='https://stackoverflow.com/questions/23792781/tf-idf-feature-weights-using-sklearn-feature-extraction-text-tfidfvectorizer'>this</a><br>\n","\n","3. Do some analysis on the Idf values and based on those values choose the low and high threshold value. Because very \n","frequent words and very very rare words don't give much information.\n","Hint - A preferable IDF range is 2-11 for model 2. <br>\n","4.Remove the low idf value and high idf value words from the train and test data. You can go through each of the\n","sentence of train and test data and include only those features(words) which are present in the defined IDF range.\n","5. Perform tokenization on the modified text data same as you have done for previous model.\n","6. Create embedding matrix for model 2 and then use the rest of the features similar to previous model.\n","7. Define the model, compile and fit the model.\n","</pre>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4EoDdZyyQmwJ"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B9V3VHPJQmwK"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4QO9YF_QmwK"},"outputs":[],"source":[""]},{"cell_type":"markdown","metadata":{"id":"K_6RhHJIQmwL"},"source":["# <font color='red'> Model-3 </font>"]},{"cell_type":"markdown","metadata":{"id":"nzhBrK7IQmwL"},"source":["<img src='https://i.imgur.com/fkQ8nGo.png'>\n","ref: https://i.imgur.com/fkQ8nGo.png"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LW4ACdeuQmwL"},"outputs":[],"source":["#in this model you can use the text vectorized data from model1 \n","#for other than text data consider the following steps\n","# you have to perform one hot encoding of categorical features. You can use onehotencoder() or countvectorizer() for the same.\n","# Stack up standardised numerical features and all the one hot encoded categorical features\n","#the input to conv1d layer is 3d, you can convert your 2d data to 3d using np.newaxis\n","# Note - deep learning models won't work with sparse features, you have to convert them to dense features before fitting in the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1HOUz-5GQmwM"},"outputs":[],"source":[""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aK2vsBelQmwM"},"outputs":[],"source":[""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.3"},"colab":{"name":"Copy of LSTM - Assignment.ipynb","provenance":[{"file_id":"1CaUcVZFafcRZlMa07B7biQTpoYbbWcID","timestamp":1654405632148}],"collapsed_sections":["NJfajcbpQmvY","dBfbDBSGlUG6","ull4A7ntR_7Y","AY0pFx4iQmvr","BpzGLz69Qmv0","xJbOczJQQmv5","rvOAcZmVQmwF","gMlQRVZdQmwI","K_6RhHJIQmwL"]}},"nbformat":4,"nbformat_minor":0}